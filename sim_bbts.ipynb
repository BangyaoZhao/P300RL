{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "# self-defined packages\n",
    "from codebase import utility\n",
    "from codebase.probmats import ProbMat\n",
    "from codebase.bcienv import SimulatedEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBTS:\n",
    "    def __init__(self, subset_size = 5):\n",
    "        self.subset_size = subset_size\n",
    "        self.alphas, self.betas = np.ones(12), np.ones(12)\n",
    "    \n",
    "    def select_indices(self):\n",
    "        thetas = np.random.beta(self.alphas, self.betas)\n",
    "        return np.argsort(-thetas)[:self.subset_size].tolist()\n",
    "    \n",
    "    def update(self, P: np.ndarray):\n",
    "        r = np.concatenate([P.sum(axis=1), P.sum(axis=0)])\n",
    "        self.alphas += r; self.betas += 1-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "mu1 = [0.9, 1.2, 1.5][2]\n",
    "############\n",
    "mu = [0, mu1]; sigma = [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'accu': [], 'util': [], 'time': []}\n",
    "#\n",
    "for rep_id in tqdm(range(50)):\n",
    "    env = SimulatedEnv(ProbMat(mu, sigma), t_flash=0.8/60, t_chr=5/60, score_mu=mu, score_sigma=sigma, p0=0)\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    for _ in range(1000):\n",
    "        bbts = BBTS()\n",
    "        for n_seq in range(45):\n",
    "            actions = bbts.select_indices()\n",
    "            for action in actions:\n",
    "                # update gp\n",
    "                obs, reward, _, _, info = env.step(action)\n",
    "            bbts.update(obs['certainty_scores'])\n",
    "            if (obs['certainty_scores'].max()>0.9):\n",
    "                break\n",
    "        #\n",
    "        obs, reward, _, _, info = env.step(12)\n",
    "        rewards.append(reward)\n",
    "    # summary results\n",
    "    n_correct, n_wrong = np.sum(np.array(rewards)>0), np.sum(np.array(rewards)<0)\n",
    "    result['accu'].append(n_correct/len(rewards))\n",
    "    result['util'].append(utility.cal_utility(n_correct, n_wrong, env.time))\n",
    "    result['time'].append(env.time/env.current_chr_id)\n",
    "{k: (np.mean(v), np.std(v)) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'accu': [], 'util': [], 'time': []}\n",
    "#\n",
    "for rep_id in tqdm(range(50)):\n",
    "    env = SimulatedEnv(ProbMat(mu, sigma), t_flash=0.2/60, t_chr=5/60, score_mu=mu, score_sigma=sigma, p0=0.5)\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    for _ in range(1000):\n",
    "        bbts = BBTS()\n",
    "        P_hist = []\n",
    "        for n_seq in range(45):\n",
    "            actions = bbts.select_indices()\n",
    "            for action in actions:\n",
    "                # update gp\n",
    "                obs, reward, _, _, info = env.step(action)\n",
    "            P_hist.append(obs['certainty_scores'])\n",
    "            if n_seq >= 1:\n",
    "                bbts.update(P_hist[-2])\n",
    "                if (P_hist[-2].max()>0.9):\n",
    "                    break\n",
    "        #\n",
    "        obs, reward, _, _, info = env.step(12)\n",
    "        rewards.append(reward)\n",
    "    # summary results\n",
    "    n_correct, n_wrong = np.sum(np.array(rewards)>0), np.sum(np.array(rewards)<0)\n",
    "    result['accu'].append(n_correct/len(rewards))\n",
    "    result['util'].append(utility.cal_utility(n_correct, n_wrong, env.time))\n",
    "    result['time'].append(env.time/env.current_chr_id)\n",
    "{k: (np.mean(v), np.std(v)) for k, v in result.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
